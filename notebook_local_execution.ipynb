{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.decomposition import PCA as skl_PCA\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA as spa_PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector, DenseVector\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Test1'\n",
    "PATH_Result = PATH+'/data/Results'\n",
    "print('PATH:'+PATH+'\\nPATH_Data:'+PATH_Data+'\\nPATH_Result: '+PATH_Result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ds_p8\")\n",
    "    .master('local')\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark context creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(PATH_Data)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=True,\n",
    "    input_shape=(224, 224, 3)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[-2].output\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the weights to the workers : (useless here)\n",
    "\n",
    "brodcast_weights = sc.broadcast(new_model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=True,\n",
    "        input_shape=(224, 224, 3)\n",
    "        )\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    new_model = Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.layers[-2].output\n",
    "        )\n",
    "\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "\n",
    "    Returns\n",
    "    - pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    Args:\n",
    "    - content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "    is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(20).select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    featurize_udf(\"content\").alias(\"features\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.show(n=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Over the features :\n",
    "\n",
    "## Spark's PCA vs Sklearn's PCA : \n",
    "- While Sklearn's option is better on small/non distributed workloads because it can be more precisely tailored, in a context of distributed computing, the sklearn's PCA would need workarounds like Ray or Dask\n",
    "- Spark's MLlib features a PCA option which, while not as customizable as its Sklearn's counterpart, is compatible by design with the formats and nature of distributed computing\n",
    "\n",
    "--> [Source](https://towardsdatascience.com/apache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82) (Medium) , partial explanation not technically on PCA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However :\n",
    "\n",
    "- k, the number of principal components, needs to be specified as input to spark's PCA - one way to do this is to guesstimate it (25% the number of features for example) - another way is to use sklearn's PCA to choose the amount of explained variance expected (let's say 80%+) - performing the PCA locally on a small sample and have a pretty good idea of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_pd = features_df.toPandas()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal k (number of components) with Sklearn :\n",
    "\n",
    "- lets aim for 95% explained variance, we can adjust the variable value if results are not satisfactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = 0.95  # if a float < 1 is passed to n_components of sklearn's pca, it will default to explained variance\n",
    "\n",
    "sk_pca = skl_PCA(n_components=explained_variance)\n",
    "\n",
    "reduced_features_values = sk_pca.fit_transform(np.stack(feature_df_pd[\"features\"].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original array size :\", np.stack(feature_df_pd[\"features\"].values).shape[1])\n",
    "print(\"reduced array size :\", reduced_features_values.shape[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like 138 principal components is a good candidate value for k, let's apply it to spark's expected k components\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting arrays to vector in the spark dataframe :\n",
    "(Sparks PCA expects vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "df_spark_vector = features_df.select(\n",
    "    features_df[\"path\"],\n",
    "    features_df[\"label\"], \n",
    "    list_to_vector_udf(features_df[\"features\"]).alias(\"features\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_vector.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_components = reduced_features_values.shape[1]  # Number of PCs for 95% explained variance in sklearn's pca\n",
    "\n",
    "spark_pca = spa_PCA(k=k_components, inputCol=\"features\")\n",
    "spark_pca.setOutputCol(\"reduced_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pca_model = spark_pca.fit(df_spark_vector)\n",
    "print(spark_pca_model.getK())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_vector = spark_pca_model.transform(dataset=df_spark_vector)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting vectors back to arrays to be readable in python as such and not dict, saving as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_array_udf = udf(\n",
    "    lambda vector: vector.toArray().tolist() \n",
    "                  if isinstance(vector, (DenseVector, SparseVector)) \n",
    "                  else vector, \n",
    "    ArrayType(FloatType())\n",
    ")\n",
    "\n",
    "df_spark_vector = df_spark_vector.withColumn(\"features\", vector_to_array_udf(df_spark_vector[\"features\"]))\n",
    "df_spark_vector = df_spark_vector.withColumn(\"reduced_features\", vector_to_array_udf(df_spark_vector[\"reduced_features\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_vector.write.mode(\"overwrite\").parquet(PATH_Result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking correct save format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, \"features\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, \"reduced_features\"].shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the initial shape of the features and the reduced features via PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
